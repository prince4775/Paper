{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261bc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cef70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stlf_torch_kit import  DataLoadeing\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, time\n",
    "from stlf_torch_kit import univariate_multi_step\n",
    "from stlf_torch_kit import SaveBestModel, PlotLossCurves, LoadModel, train, TestModel, BatchGenerator, results\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffabf9c1-c91d-476c-bb1e-90968a07d31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc31f46",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2a9dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84907, 21), (24259, 21), (12130, 21))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dataset =r'C:\\Users\\Personal\\Desktop\\students\\AEP\\AEP' #Edit\n",
    "path_tr = os.path.join(path_dataset, 'AEP_train.csv')\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.values\n",
    "path_v = os.path.join(path_dataset, 'AEP_validation.csv')\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.values\n",
    "path_te = os.path.join(path_dataset, 'AEP_test.csv')\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.values \n",
    "\n",
    "path_scaler = os.path.join(path_dataset, 'AEP_Scaler.pkl')\n",
    "scaler         = pickle.load(open(path_scaler, 'rb'))\n",
    "\n",
    "train_set.shape, validation_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293ad9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.31334519386291504 sec\n"
     ]
    }
   ],
   "source": [
    "time_steps=24 #look back or sequence length, lag, window size #Edit\n",
    "target_len = 1 #how much steps do you wana forecast #Edit\n",
    "start = time.time()\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=0,target_len=target_len)\n",
    "validation_X, validation_y = univariate_multi_step(validation_set, time_steps, target_col=0,target_len=target_len)\n",
    "test_X, test_y = univariate_multi_step(test_set, time_steps, target_col=0,target_len=target_len)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc241223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12105, 24, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52558d",
   "metadata": {},
   "source": [
    "# Proposed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28011352",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8487fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# major edit\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.lstm = nn.LSTM(1, 20, 1, batch_first=True).to(self.device) # num-features, node/units, num-layers\n",
    "        self.fc = nn.Linear(20, 1).to(self.device) #(in,out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x) # _=(h,c)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e842fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LSTMModel                                [64, 1]                   --\n",
      "├─LSTM: 1-1                              [64, 24, 20]              1,840\n",
      "├─Linear: 1-2                            [64, 1]                   21\n",
      "==========================================================================================\n",
      "Total params: 1,861\n",
      "Trainable params: 1,861\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.83\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.25\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.26\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel()\n",
    "print(summary(model, input_size=(64, 24, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533489a",
   "metadata": {},
   "source": [
    "#### CNN\n",
    "<table style=\"border: 1px solid black; border-collapse: collapse; width: 100%;\">\n",
    "    <thead>\n",
    "        <tr style=\"background-color: #f2f2f2;\">\n",
    "            <th style=\"border: 1px solid black; padding: 8px;\">Layer</th>\n",
    "            <th style=\"border: 1px solid black; padding: 8px;\">Expected Input Shape</th>\n",
    "            <th style=\"border: 1px solid black; padding: 8px;\">Common Adjustments Needed</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black; padding: 8px;\">Conv1d</td>\n",
    "            <td style=\"border: 1px solid black; padding: 8px;\">(batch_size, channels, sequence_length)</td>\n",
    "            <td style=\"border: 1px solid black; padding: 8px;\">Permute input from (batch, seq, feat) to (batch, feat, seq)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black; padding: 8px;\">LSTM</td>\n",
    "            <td style=\"border: 1px solid black; padding: 8px;\">(batch_size, sequence_length, features)</td>\n",
    "            <td style=\"border: 1px solid black; padding: 8px;\">No permutation if batch_first=True</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7fa333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3).to(self.device)\n",
    "        self.fc = nn.Linear(32*22, 1).to(self.device) #32 channels ao 22 features dy chy kernel size 3 na bad kam shwal\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))  \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.fc(x) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5e53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CNN1DModel                               [64, 1]                   --\n",
      "├─Conv1d: 1-1                            [64, 32, 22]              128\n",
      "├─Linear: 1-2                            [64, 1]                   705\n",
      "==========================================================================================\n",
      "Total params: 833\n",
      "Trainable params: 833\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.23\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.37\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = CNN1DModel()\n",
    "print(summary(model, input_size=(64, 24, 1)))\n",
    "# summary(model, input_size=(64, 24, 1), col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b859f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.conv1 = nn.Conv1d(in_channels=21, out_channels=64, kernel_size=3).to(self.device)\n",
    "       \n",
    "        self.lstm = nn.LSTM(64, 55, 3, batch_first=True).to(self.device)\n",
    "        self.fc = nn.Linear(55, 25).to(self.device) \n",
    "        self.fc1 = nn.Linear(25, 1).to(self.device) \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x) # _=(h,c)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.fc1(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61459259",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CNNLSTMModel                             [64, 1]                   --\n",
      "├─Conv1d: 1-1                            [64, 64, 22]              4,096\n",
      "├─LSTM: 1-2                              [64, 22, 55]              75,900\n",
      "├─Linear: 1-3                            [64, 25]                  1,400\n",
      "├─Linear: 1-4                            [64, 1]                   26\n",
      "==========================================================================================\n",
      "Total params: 81,422\n",
      "Trainable params: 81,422\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 112.73\n",
      "==========================================================================================\n",
      "Input size (MB): 0.13\n",
      "Forward/backward pass size (MB): 1.35\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 1.81\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = CNNLSTMModel()\n",
    "print(summary(model, input_size=(64, 24, 21)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9a813",
   "metadata": {},
   "source": [
    "#### Mostly Cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1912c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOSTLY_CITED(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.lstm = nn.LSTM(1, 20, 2, batch_first=True).to(self.device)\n",
    "        self.fc = nn.Linear(20, 1).to(self.device) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x) # _=(h,c)\n",
    "        out = torch.sigmoid(self.fc(out[:, -1, :]) )\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7491daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659def4",
   "metadata": {},
   "source": [
    "# instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dbb6ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNLSTMModel(\n",
       "  (conv1): Conv1d(21, 64, kernel_size=(3,), stride=(1,))\n",
       "  (lstm): LSTM(64, 55, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=55, out_features=25, bias=True)\n",
       "  (fc1): Linear(in_features=25, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1f740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNLSTMModel()#Edit\n",
    "criterion = nn.MSELoss() #Edit, don't change\n",
    "\n",
    "save_best_model = SaveBestModel()\n",
    "Plot_Loss=PlotLossCurves()\n",
    "load_model=LoadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a217f",
   "metadata": {},
   "source": [
    "# Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9867523",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001 # Edit\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b55ecf",
   "metadata": {},
   "source": [
    "# Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53206b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_model_device(model):\n",
    "    return next(model.parameters()).device\n",
    "device = get_model_device(model)\n",
    "print(\"Model is on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779171af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8fafebb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa963e",
   "metadata": {},
   "source": [
    "#### Path & other Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3924df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "num_epochs = 20 #Edit\n",
    "best_model_path=r'D:\\chk6'+str('\\\\') #Edit\n",
    "fig_path=r'D:\\chk6' #Edit\n",
    "train_data_loader, validation_data_loader, test_data_loader = DataLoadeing(train_X ,\n",
    "                                                                           train_y, \n",
    "                                                                           validation_X, \n",
    "                                                                           validation_y, \n",
    "                                                                           test_X, \n",
    "                                                                           test_y, \n",
    "                                                                           batch_size=32) #Batch_Size Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460874b",
   "metadata": {},
   "source": [
    "#### Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b17e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # Edit, for Now Don't  Change\n",
    "save_best_model = SaveBestModel()\n",
    "Plot_Loss=PlotLossCurves()\n",
    "load_model=LoadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bee91",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37aa742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [2653/2653], Training Loss: 0.0092\n",
      "Epoch [1/20], Step [758/758], Val Loss: 0.0109\n",
      "\n",
      "Saving best model for epoch: 1\n",
      " at D:\\chk6\\1best_model.pth\n",
      "Epoch [2/20], Step [2653/2653], Training Loss: 0.0043\n",
      "Epoch [2/20], Step [758/758], Val Loss: 0.0052\n",
      "\n",
      "Saving best model for epoch: 2\n",
      " at D:\\chk6\\2best_model.pth\n",
      "Epoch [3/20], Step [2653/2653], Training Loss: 0.0010\n",
      "Epoch [3/20], Step [758/758], Val Loss: 0.0032\n",
      "\n",
      "Saving best model for epoch: 3\n",
      " at D:\\chk6\\3best_model.pth\n",
      "Epoch [4/20], Step [2653/2653], Training Loss: 0.0004\n",
      "Epoch [4/20], Step [758/758], Val Loss: 0.0005\n",
      "\n",
      "Saving best model for epoch: 4\n",
      " at D:\\chk6\\4best_model.pth\n",
      "Epoch [5/20], Step [2653/2653], Training Loss: 0.0003\n",
      "Epoch [5/20], Step [758/758], Val Loss: 0.0003\n",
      "\n",
      "Saving best model for epoch: 5\n",
      " at D:\\chk6\\5best_model.pth\n",
      "Epoch [6/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [6/20], Step [758/758], Val Loss: 0.0003\n",
      "Epoch [7/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [7/20], Step [758/758], Val Loss: 0.0003\n",
      "\n",
      "Saving best model for epoch: 7\n",
      " at D:\\chk6\\7best_model.pth\n",
      "Epoch [8/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [8/20], Step [758/758], Val Loss: 0.0002\n",
      "\n",
      "Saving best model for epoch: 8\n",
      " at D:\\chk6\\8best_model.pth\n",
      "Epoch [9/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [9/20], Step [758/758], Val Loss: 0.0002\n",
      "Epoch [10/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [10/20], Step [758/758], Val Loss: 0.0002\n",
      "\n",
      "Saving best model for epoch: 10\n",
      " at D:\\chk6\\10best_model.pth\n",
      "Epoch [11/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [11/20], Step [758/758], Val Loss: 0.0002\n",
      "Epoch [12/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [12/20], Step [758/758], Val Loss: 0.0003\n",
      "Epoch [13/20], Step [2653/2653], Training Loss: 0.0002\n",
      "Epoch [13/20], Step [758/758], Val Loss: 0.0005\n",
      "Epoch [14/20], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [14/20], Step [758/758], Val Loss: 0.0003\n",
      "Epoch [15/20], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [15/20], Step [758/758], Val Loss: 0.0004\n",
      "Epoch [16/20], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [16/20], Step [758/758], Val Loss: 0.0005\n",
      "Epoch [17/20], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [17/20], Step [758/758], Val Loss: 0.0002\n",
      "Epoch [18/20], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [18/20], Step [758/758], Val Loss: 0.0004\n",
      "Epoch [19/20], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [19/20], Step [758/758], Val Loss: 0.0003\n",
      "Epoch [20/20], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [20/20], Step [758/758], Val Loss: 0.0002\n",
      "Time Consumed 246.00577998161316 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c897f0",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f2d45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8e071d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 0.001\n",
      "Time Consumed 0.8097715377807617 sec\n",
      "Mean Absolute Error (MAE): 152.4\n",
      "Median Absolute Error (MedAE): 120.78\n",
      "Mean Squared Error (MSE): 40403.06\n",
      "Root Mean Squared Error (RMSE): 201.01\n",
      "Mean Absolute Percentage Error (MAPE): 1.04 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.83 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk6\\10best_model.pth' # Edit\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)\n",
    "\n",
    "# MAPE, MAE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ed24f",
   "metadata": {},
   "source": [
    "# Fine Tunning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42338ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "lr=0.0001\n",
    "Batch_size = True #batch size maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaffc7c",
   "metadata": {},
   "source": [
    "#### Load Model for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d00f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 0.0001\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=32,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7d8432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [11/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 11\n",
      " at D:\\chk6\\11best_model.pth\n",
      "Epoch [12/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [12/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 12\n",
      " at D:\\chk6\\12best_model.pth\n",
      "Epoch [13/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [13/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 13\n",
      " at D:\\chk6\\13best_model.pth\n",
      "Epoch [14/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [14/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 14\n",
      " at D:\\chk6\\14best_model.pth\n",
      "Epoch [15/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [15/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 15\n",
      " at D:\\chk6\\15best_model.pth\n",
      "Epoch [16/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [16/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 16\n",
      " at D:\\chk6\\16best_model.pth\n",
      "Epoch [17/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [17/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 17\n",
      " at D:\\chk6\\17best_model.pth\n",
      "Epoch [18/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [18/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 18\n",
      " at D:\\chk6\\18best_model.pth\n",
      "Epoch [19/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [19/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 19\n",
      " at D:\\chk6\\19best_model.pth\n",
      "Epoch [20/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [20/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 20\n",
      " at D:\\chk6\\20best_model.pth\n",
      "Epoch [21/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [21/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 21\n",
      " at D:\\chk6\\21best_model.pth\n",
      "Epoch [22/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [22/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 22\n",
      " at D:\\chk6\\22best_model.pth\n",
      "Epoch [23/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [23/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 23\n",
      " at D:\\chk6\\23best_model.pth\n",
      "Epoch [24/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [24/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 24\n",
      " at D:\\chk6\\24best_model.pth\n",
      "Epoch [25/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [25/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 25\n",
      " at D:\\chk6\\25best_model.pth\n",
      "Epoch [26/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [26/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 26\n",
      " at D:\\chk6\\26best_model.pth\n",
      "Epoch [27/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [27/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 27\n",
      " at D:\\chk6\\27best_model.pth\n",
      "Epoch [28/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [28/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 28\n",
      " at D:\\chk6\\28best_model.pth\n",
      "Epoch [29/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [29/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [30/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [30/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 30\n",
      " at D:\\chk6\\30best_model.pth\n",
      "Epoch [31/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [31/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 31\n",
      " at D:\\chk6\\31best_model.pth\n",
      "Epoch [32/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [32/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 32\n",
      " at D:\\chk6\\32best_model.pth\n",
      "Epoch [33/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [33/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 33\n",
      " at D:\\chk6\\33best_model.pth\n",
      "Epoch [34/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [34/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 34\n",
      " at D:\\chk6\\34best_model.pth\n",
      "Epoch [35/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [35/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 35\n",
      " at D:\\chk6\\35best_model.pth\n",
      "Epoch [36/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [36/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 36\n",
      " at D:\\chk6\\36best_model.pth\n",
      "Epoch [37/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [37/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 37\n",
      " at D:\\chk6\\37best_model.pth\n",
      "Epoch [38/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [38/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 38\n",
      " at D:\\chk6\\38best_model.pth\n",
      "Epoch [39/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [39/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 39\n",
      " at D:\\chk6\\39best_model.pth\n",
      "Epoch [40/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [40/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 40\n",
      " at D:\\chk6\\40best_model.pth\n",
      "Epoch [41/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [41/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 41\n",
      " at D:\\chk6\\41best_model.pth\n",
      "Epoch [42/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [42/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [43/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [43/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 43\n",
      " at D:\\chk6\\43best_model.pth\n",
      "Epoch [44/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [44/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 44\n",
      " at D:\\chk6\\44best_model.pth\n",
      "Epoch [45/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [45/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 45\n",
      " at D:\\chk6\\45best_model.pth\n",
      "Epoch [46/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [46/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [47/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [47/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 47\n",
      " at D:\\chk6\\47best_model.pth\n",
      "Epoch [48/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [48/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 48\n",
      " at D:\\chk6\\48best_model.pth\n",
      "Epoch [49/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [49/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 49\n",
      " at D:\\chk6\\49best_model.pth\n",
      "Epoch [50/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [50/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 50\n",
      " at D:\\chk6\\50best_model.pth\n",
      "Epoch [51/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [51/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 51\n",
      " at D:\\chk6\\51best_model.pth\n",
      "Epoch [52/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [52/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [53/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [53/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 53\n",
      " at D:\\chk6\\53best_model.pth\n",
      "Epoch [54/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [54/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 54\n",
      " at D:\\chk6\\54best_model.pth\n",
      "Epoch [55/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [55/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 55\n",
      " at D:\\chk6\\55best_model.pth\n",
      "Epoch [56/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [56/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 56\n",
      " at D:\\chk6\\56best_model.pth\n",
      "Epoch [57/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [57/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 57\n",
      " at D:\\chk6\\57best_model.pth\n",
      "Epoch [58/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [58/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [59/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [59/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [60/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [60/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 60\n",
      " at D:\\chk6\\60best_model.pth\n",
      "Epoch [61/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [61/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [62/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [62/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 62\n",
      " at D:\\chk6\\62best_model.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [63/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [64/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [64/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 64\n",
      " at D:\\chk6\\64best_model.pth\n",
      "Epoch [65/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [65/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [66/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [66/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 66\n",
      " at D:\\chk6\\66best_model.pth\n",
      "Epoch [67/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [67/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 67\n",
      " at D:\\chk6\\67best_model.pth\n",
      "Epoch [68/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [68/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [69/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [69/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 69\n",
      " at D:\\chk6\\69best_model.pth\n",
      "Epoch [70/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [70/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [71/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [71/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 71\n",
      " at D:\\chk6\\71best_model.pth\n",
      "Epoch [72/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [72/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [73/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [73/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [74/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [74/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [75/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [75/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 75\n",
      " at D:\\chk6\\75best_model.pth\n",
      "Epoch [76/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [76/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [77/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [77/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 77\n",
      " at D:\\chk6\\77best_model.pth\n",
      "Epoch [78/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [78/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [79/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [79/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [80/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [80/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [81/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [81/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [82/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [82/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [83/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [83/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [84/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [84/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 84\n",
      " at D:\\chk6\\84best_model.pth\n",
      "Epoch [85/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [85/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [86/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [86/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [87/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [87/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [88/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [88/110], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 88\n",
      " at D:\\chk6\\88best_model.pth\n",
      "Epoch [89/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [89/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [90/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [90/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [91/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [91/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [92/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [92/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [93/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [93/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [94/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [94/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [95/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [95/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [96/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [96/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [97/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [97/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [98/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [98/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [99/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [99/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [100/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [100/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [101/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [101/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [102/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [102/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [103/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [103/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [104/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [104/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [105/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [105/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [106/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [106/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [107/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [107/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [108/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [108/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [109/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [109/110], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [110/110], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [110/110], Step [758/758], Val Loss: 0.0001\n",
      "Time Consumed 1249.0271835327148 sec\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19971578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 0.0001\n",
      "Time Consumed 0.33310914039611816 sec\n",
      "Mean Absolute Error (MAE): 91.52\n",
      "Median Absolute Error (MedAE): 73.46\n",
      "Mean Squared Error (MSE): 14804.34\n",
      "Root Mean Squared Error (RMSE): 121.67\n",
      "Mean Absolute Percentage Error (MAPE): 0.62 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.51 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk6\\88best_model.pth'\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b5c72",
   "metadata": {},
   "source": [
    "# Fine Tunning 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8855adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "lr = 0.00001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk6\\88best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1a0f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfc79445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/108], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [89/108], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [90/108], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [90/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 90\n",
      " at D:\\chk6\\90best_model.pth\n",
      "Epoch [91/108], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [91/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 91\n",
      " at D:\\chk6\\91best_model.pth\n",
      "Epoch [92/108], Step [2653/2653], Training Loss: 0.0001\n",
      "Epoch [92/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 92\n",
      " at D:\\chk6\\92best_model.pth\n",
      "Epoch [93/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [93/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 93\n",
      " at D:\\chk6\\93best_model.pth\n",
      "Epoch [94/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [94/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 94\n",
      " at D:\\chk6\\94best_model.pth\n",
      "Epoch [95/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [95/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 95\n",
      " at D:\\chk6\\95best_model.pth\n",
      "Epoch [96/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [96/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 96\n",
      " at D:\\chk6\\96best_model.pth\n",
      "Epoch [97/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [97/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 97\n",
      " at D:\\chk6\\97best_model.pth\n",
      "Epoch [98/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [98/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 98\n",
      " at D:\\chk6\\98best_model.pth\n",
      "Epoch [99/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [99/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 99\n",
      " at D:\\chk6\\99best_model.pth\n",
      "Epoch [100/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [100/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 100\n",
      " at D:\\chk6\\100best_model.pth\n",
      "Epoch [101/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [101/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 101\n",
      " at D:\\chk6\\101best_model.pth\n",
      "Epoch [102/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [102/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 102\n",
      " at D:\\chk6\\102best_model.pth\n",
      "Epoch [103/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [103/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 103\n",
      " at D:\\chk6\\103best_model.pth\n",
      "Epoch [104/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [104/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 104\n",
      " at D:\\chk6\\104best_model.pth\n",
      "Epoch [105/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [105/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 105\n",
      " at D:\\chk6\\105best_model.pth\n",
      "Epoch [106/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [106/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 106\n",
      " at D:\\chk6\\106best_model.pth\n",
      "Epoch [107/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [107/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 107\n",
      " at D:\\chk6\\107best_model.pth\n",
      "Epoch [108/108], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [108/108], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 108\n",
      " at D:\\chk6\\108best_model.pth\n",
      "Time Consumed 252.91170525550842 sec\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "403a487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.29328155517578125 sec\n",
      "Mean Absolute Error (MAE): 91.01\n",
      "Median Absolute Error (MedAE): 71.54\n",
      "Mean Squared Error (MSE): 14858.5\n",
      "Root Mean Squared Error (RMSE): 121.9\n",
      "Mean Absolute Percentage Error (MAPE): 0.62 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.5 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk6\\108best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2db74c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "#lr = 0.0001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk6\\108best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dab68625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba9a314b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [109/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [109/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 109\n",
      " at D:\\chk6\\109best_model.pth\n",
      "Epoch [110/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [110/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 110\n",
      " at D:\\chk6\\110best_model.pth\n",
      "Epoch [111/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [111/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 111\n",
      " at D:\\chk6\\111best_model.pth\n",
      "Epoch [112/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [112/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 112\n",
      " at D:\\chk6\\112best_model.pth\n",
      "Epoch [113/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [113/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 113\n",
      " at D:\\chk6\\113best_model.pth\n",
      "Epoch [114/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [114/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 114\n",
      " at D:\\chk6\\114best_model.pth\n",
      "Epoch [115/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [115/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 115\n",
      " at D:\\chk6\\115best_model.pth\n",
      "Epoch [116/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [116/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 116\n",
      " at D:\\chk6\\116best_model.pth\n",
      "Epoch [117/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [117/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 117\n",
      " at D:\\chk6\\117best_model.pth\n",
      "Epoch [118/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [118/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 118\n",
      " at D:\\chk6\\118best_model.pth\n",
      "Epoch [119/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [119/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 119\n",
      " at D:\\chk6\\119best_model.pth\n",
      "Epoch [120/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [120/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 120\n",
      " at D:\\chk6\\120best_model.pth\n",
      "Epoch [121/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [121/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 121\n",
      " at D:\\chk6\\121best_model.pth\n",
      "Epoch [122/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [122/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 122\n",
      " at D:\\chk6\\122best_model.pth\n",
      "Epoch [123/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [123/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 123\n",
      " at D:\\chk6\\123best_model.pth\n",
      "Epoch [124/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [124/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 124\n",
      " at D:\\chk6\\124best_model.pth\n",
      "Epoch [125/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [125/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 125\n",
      " at D:\\chk6\\125best_model.pth\n",
      "Epoch [126/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [126/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 126\n",
      " at D:\\chk6\\126best_model.pth\n",
      "Epoch [127/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [127/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 127\n",
      " at D:\\chk6\\127best_model.pth\n",
      "Epoch [128/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [128/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 128\n",
      " at D:\\chk6\\128best_model.pth\n",
      "Epoch [129/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [129/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 129\n",
      " at D:\\chk6\\129best_model.pth\n",
      "Epoch [130/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [130/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 130\n",
      " at D:\\chk6\\130best_model.pth\n",
      "Epoch [131/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [131/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 131\n",
      " at D:\\chk6\\131best_model.pth\n",
      "Epoch [132/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [132/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 132\n",
      " at D:\\chk6\\132best_model.pth\n",
      "Epoch [133/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [133/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 133\n",
      " at D:\\chk6\\133best_model.pth\n",
      "Epoch [134/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [134/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 134\n",
      " at D:\\chk6\\134best_model.pth\n",
      "Epoch [135/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [135/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 135\n",
      " at D:\\chk6\\135best_model.pth\n",
      "Epoch [136/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [136/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 136\n",
      " at D:\\chk6\\136best_model.pth\n",
      "Epoch [137/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [137/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 137\n",
      " at D:\\chk6\\137best_model.pth\n",
      "Epoch [138/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [138/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 138\n",
      " at D:\\chk6\\138best_model.pth\n",
      "Epoch [139/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [139/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 139\n",
      " at D:\\chk6\\139best_model.pth\n",
      "Epoch [140/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [140/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 140\n",
      " at D:\\chk6\\140best_model.pth\n",
      "Epoch [141/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [141/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 141\n",
      " at D:\\chk6\\141best_model.pth\n",
      "Epoch [142/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [142/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 142\n",
      " at D:\\chk6\\142best_model.pth\n",
      "Epoch [143/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [143/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 143\n",
      " at D:\\chk6\\143best_model.pth\n",
      "Epoch [144/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [144/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 144\n",
      " at D:\\chk6\\144best_model.pth\n",
      "Epoch [145/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [145/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 145\n",
      " at D:\\chk6\\145best_model.pth\n",
      "Epoch [146/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [146/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 146\n",
      " at D:\\chk6\\146best_model.pth\n",
      "Epoch [147/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [147/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 147\n",
      " at D:\\chk6\\147best_model.pth\n",
      "Epoch [148/148], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [148/148], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 148\n",
      " at D:\\chk6\\148best_model.pth\n",
      "Time Consumed 570.459014415741 sec\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e08ead66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.7108466625213623 sec\n",
      "Mean Absolute Error (MAE): 90.44\n",
      "Median Absolute Error (MedAE): 70.79\n",
      "Mean Squared Error (MSE): 14699.12\n",
      "Root Mean Squared Error (RMSE): 121.24\n",
      "Mean Absolute Percentage Error (MAPE): 0.62 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.49 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk6\\148best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6930fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85d1b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "#lr = 0.00001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk6\\148best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "261fc4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52effded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [149/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [149/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 149\n",
      " at D:\\chk6\\149best_model.pth\n",
      "Epoch [150/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [150/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 150\n",
      " at D:\\chk6\\150best_model.pth\n",
      "Epoch [151/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [151/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 151\n",
      " at D:\\chk6\\151best_model.pth\n",
      "Epoch [152/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [152/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 152\n",
      " at D:\\chk6\\152best_model.pth\n",
      "Epoch [153/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [153/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 153\n",
      " at D:\\chk6\\153best_model.pth\n",
      "Epoch [154/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [154/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 154\n",
      " at D:\\chk6\\154best_model.pth\n",
      "Epoch [155/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [155/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 155\n",
      " at D:\\chk6\\155best_model.pth\n",
      "Epoch [156/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [156/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 156\n",
      " at D:\\chk6\\156best_model.pth\n",
      "Epoch [157/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [157/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 157\n",
      " at D:\\chk6\\157best_model.pth\n",
      "Epoch [158/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [158/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 158\n",
      " at D:\\chk6\\158best_model.pth\n",
      "Epoch [159/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [159/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 159\n",
      " at D:\\chk6\\159best_model.pth\n",
      "Epoch [160/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [160/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 160\n",
      " at D:\\chk6\\160best_model.pth\n",
      "Epoch [161/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [161/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 161\n",
      " at D:\\chk6\\161best_model.pth\n",
      "Epoch [162/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [162/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 162\n",
      " at D:\\chk6\\162best_model.pth\n",
      "Epoch [163/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [163/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 163\n",
      " at D:\\chk6\\163best_model.pth\n",
      "Epoch [164/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [164/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 164\n",
      " at D:\\chk6\\164best_model.pth\n",
      "Epoch [165/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [165/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 165\n",
      " at D:\\chk6\\165best_model.pth\n",
      "Epoch [166/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [166/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 166\n",
      " at D:\\chk6\\166best_model.pth\n",
      "Epoch [167/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [167/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 167\n",
      " at D:\\chk6\\167best_model.pth\n",
      "Epoch [168/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [168/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 168\n",
      " at D:\\chk6\\168best_model.pth\n",
      "Epoch [169/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [169/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 169\n",
      " at D:\\chk6\\169best_model.pth\n",
      "Epoch [170/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [170/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 170\n",
      " at D:\\chk6\\170best_model.pth\n",
      "Epoch [171/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [171/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 171\n",
      " at D:\\chk6\\171best_model.pth\n",
      "Epoch [172/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [172/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 172\n",
      " at D:\\chk6\\172best_model.pth\n",
      "Epoch [173/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [173/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 173\n",
      " at D:\\chk6\\173best_model.pth\n",
      "Epoch [174/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [174/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 174\n",
      " at D:\\chk6\\174best_model.pth\n",
      "Epoch [175/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [175/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 175\n",
      " at D:\\chk6\\175best_model.pth\n",
      "Epoch [176/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [176/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 176\n",
      " at D:\\chk6\\176best_model.pth\n",
      "Epoch [177/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [177/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 177\n",
      " at D:\\chk6\\177best_model.pth\n",
      "Epoch [178/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [178/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 178\n",
      " at D:\\chk6\\178best_model.pth\n",
      "Epoch [179/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [179/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 179\n",
      " at D:\\chk6\\179best_model.pth\n",
      "Epoch [180/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [180/188], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [181/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [181/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 181\n",
      " at D:\\chk6\\181best_model.pth\n",
      "Epoch [182/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [182/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 182\n",
      " at D:\\chk6\\182best_model.pth\n",
      "Epoch [183/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [183/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 183\n",
      " at D:\\chk6\\183best_model.pth\n",
      "Epoch [184/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [184/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 184\n",
      " at D:\\chk6\\184best_model.pth\n",
      "Epoch [185/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [185/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 185\n",
      " at D:\\chk6\\185best_model.pth\n",
      "Epoch [186/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [186/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 186\n",
      " at D:\\chk6\\186best_model.pth\n",
      "Epoch [187/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [187/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 187\n",
      " at D:\\chk6\\187best_model.pth\n",
      "Epoch [188/188], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [188/188], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 188\n",
      " at D:\\chk6\\188best_model.pth\n",
      "Time Consumed 522.1760857105255 sec\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "989a1be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.32709288597106934 sec\n",
      "Mean Absolute Error (MAE): 90.16\n",
      "Median Absolute Error (MedAE): 70.6\n",
      "Mean Squared Error (MSE): 14619.34\n",
      "Root Mean Squared Error (RMSE): 120.91\n",
      "Mean Absolute Percentage Error (MAPE): 0.61 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.49 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk6\\188best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7bc256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f1cc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "#lr = 0.0001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk6\\188best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f2ef1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90ee7223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [189/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [189/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 189\n",
      " at D:\\chk6\\189best_model.pth\n",
      "Epoch [190/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [190/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 190\n",
      " at D:\\chk6\\190best_model.pth\n",
      "Epoch [191/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [191/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 191\n",
      " at D:\\chk6\\191best_model.pth\n",
      "Epoch [192/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [192/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 192\n",
      " at D:\\chk6\\192best_model.pth\n",
      "Epoch [193/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [193/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 193\n",
      " at D:\\chk6\\193best_model.pth\n",
      "Epoch [194/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [194/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 194\n",
      " at D:\\chk6\\194best_model.pth\n",
      "Epoch [195/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [195/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 195\n",
      " at D:\\chk6\\195best_model.pth\n",
      "Epoch [196/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [196/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 196\n",
      " at D:\\chk6\\196best_model.pth\n",
      "Epoch [197/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [197/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 197\n",
      " at D:\\chk6\\197best_model.pth\n",
      "Epoch [198/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [198/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 198\n",
      " at D:\\chk6\\198best_model.pth\n",
      "Epoch [199/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [199/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 199\n",
      " at D:\\chk6\\199best_model.pth\n",
      "Epoch [200/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [200/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 200\n",
      " at D:\\chk6\\200best_model.pth\n",
      "Epoch [201/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [201/248], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [202/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [202/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 202\n",
      " at D:\\chk6\\202best_model.pth\n",
      "Epoch [203/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [203/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 203\n",
      " at D:\\chk6\\203best_model.pth\n",
      "Epoch [204/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [204/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 204\n",
      " at D:\\chk6\\204best_model.pth\n",
      "Epoch [205/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [205/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 205\n",
      " at D:\\chk6\\205best_model.pth\n",
      "Epoch [206/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [206/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 206\n",
      " at D:\\chk6\\206best_model.pth\n",
      "Epoch [207/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [207/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 207\n",
      " at D:\\chk6\\207best_model.pth\n",
      "Epoch [208/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [208/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 208\n",
      " at D:\\chk6\\208best_model.pth\n",
      "Epoch [209/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [209/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 209\n",
      " at D:\\chk6\\209best_model.pth\n",
      "Epoch [210/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [210/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 210\n",
      " at D:\\chk6\\210best_model.pth\n",
      "Epoch [211/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [211/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 211\n",
      " at D:\\chk6\\211best_model.pth\n",
      "Epoch [212/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [212/248], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [213/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [213/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 213\n",
      " at D:\\chk6\\213best_model.pth\n",
      "Epoch [214/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [214/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 214\n",
      " at D:\\chk6\\214best_model.pth\n",
      "Epoch [215/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [215/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 215\n",
      " at D:\\chk6\\215best_model.pth\n",
      "Epoch [216/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [216/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 216\n",
      " at D:\\chk6\\216best_model.pth\n",
      "Epoch [217/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [217/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 217\n",
      " at D:\\chk6\\217best_model.pth\n",
      "Epoch [218/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [218/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 218\n",
      " at D:\\chk6\\218best_model.pth\n",
      "Epoch [219/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [219/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 219\n",
      " at D:\\chk6\\219best_model.pth\n",
      "Epoch [220/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [220/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 220\n",
      " at D:\\chk6\\220best_model.pth\n",
      "Epoch [221/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [221/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 221\n",
      " at D:\\chk6\\221best_model.pth\n",
      "Epoch [222/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [222/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 222\n",
      " at D:\\chk6\\222best_model.pth\n",
      "Epoch [223/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [223/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 223\n",
      " at D:\\chk6\\223best_model.pth\n",
      "Epoch [224/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [224/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 224\n",
      " at D:\\chk6\\224best_model.pth\n",
      "Epoch [225/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [225/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 225\n",
      " at D:\\chk6\\225best_model.pth\n",
      "Epoch [226/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [226/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 226\n",
      " at D:\\chk6\\226best_model.pth\n",
      "Epoch [227/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [227/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 227\n",
      " at D:\\chk6\\227best_model.pth\n",
      "Epoch [228/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [228/248], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [229/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [229/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 229\n",
      " at D:\\chk6\\229best_model.pth\n",
      "Epoch [230/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [230/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 230\n",
      " at D:\\chk6\\230best_model.pth\n",
      "Epoch [231/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [231/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 231\n",
      " at D:\\chk6\\231best_model.pth\n",
      "Epoch [232/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [232/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 232\n",
      " at D:\\chk6\\232best_model.pth\n",
      "Epoch [233/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [233/248], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [234/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [234/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 234\n",
      " at D:\\chk6\\234best_model.pth\n",
      "Epoch [235/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [235/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 235\n",
      " at D:\\chk6\\235best_model.pth\n",
      "Epoch [236/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [236/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 236\n",
      " at D:\\chk6\\236best_model.pth\n",
      "Epoch [237/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [237/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 237\n",
      " at D:\\chk6\\237best_model.pth\n",
      "Epoch [238/248], Step [2653/2653], Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238/248], Step [18/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [19/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [20/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [21/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [22/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [23/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [24/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [25/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [26/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [27/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [28/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [29/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [30/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [31/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [32/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [33/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [34/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [35/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [36/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [37/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [38/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [39/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [40/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [41/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [42/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [43/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [44/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [45/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [46/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [47/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [48/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [49/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [50/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [51/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [52/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [53/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [54/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [55/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [56/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [57/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [58/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [59/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [60/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [61/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [62/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [63/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [64/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [65/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [66/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [67/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [68/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [69/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [70/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [71/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [72/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [73/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [74/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [75/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [76/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [77/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [78/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [79/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [80/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [81/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [82/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [83/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [84/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [85/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [86/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [87/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [88/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [89/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [90/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [91/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [92/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [93/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [94/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [95/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [96/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [97/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [98/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [99/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [100/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [101/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [102/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [103/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [104/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [105/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [106/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [107/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [108/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [109/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [110/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [111/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [112/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [113/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [114/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [115/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [116/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [117/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [118/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [119/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [120/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [121/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [122/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [123/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [124/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [125/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [126/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [127/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [128/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [129/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [130/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [131/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [132/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [133/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [134/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [135/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [136/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [137/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [138/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [139/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [140/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [141/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [142/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [143/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [144/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [145/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [146/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [147/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [148/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [149/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [150/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [151/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [152/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [153/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [154/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [155/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [156/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [157/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [158/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [159/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [160/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [161/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [162/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [163/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [164/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [165/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [166/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [167/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [168/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [169/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [170/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [171/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [172/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [173/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [174/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [175/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [176/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [177/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [178/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [179/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [180/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [181/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [182/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [183/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [184/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [185/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [186/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [187/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [188/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [189/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [190/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [191/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [192/758], Val Loss: 0.0002\r",
      "Epoch [238/248], Step [193/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [194/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [195/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [196/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [197/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [198/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [199/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [200/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [201/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [202/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [203/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [204/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [205/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [206/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [207/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [208/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [209/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [210/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [211/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [212/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [213/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [214/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [215/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [216/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [217/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [218/758], Val Loss: 0.0000\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238/248], Step [219/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [220/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [221/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [222/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [223/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [224/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [225/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [226/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [227/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [228/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [229/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [230/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [231/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [232/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [233/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [234/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [235/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [236/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [237/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [238/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [239/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [240/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [241/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [242/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [243/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [244/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [245/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [246/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [247/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [248/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [249/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [250/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [251/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [252/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [253/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [254/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [255/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [256/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [257/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [258/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [259/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [260/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [261/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [262/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [263/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [264/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [265/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [266/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [267/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [268/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [269/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [270/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [271/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [272/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [273/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [274/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [275/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [276/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [277/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [278/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [279/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [280/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [281/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [282/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [283/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [284/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [285/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [286/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [287/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [288/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [289/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [290/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [291/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [292/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [293/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [294/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [295/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [296/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [297/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [298/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [299/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [300/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [301/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [302/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [303/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [304/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [305/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [306/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [307/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [308/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [309/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [310/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [311/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [312/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [313/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [314/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [315/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [316/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [317/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [318/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [319/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [320/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [321/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [322/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [323/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [324/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [325/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [326/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [327/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [328/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [329/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [330/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [331/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [332/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [333/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [334/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [335/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [336/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [337/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [338/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [339/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [340/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [341/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [342/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [343/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [344/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [345/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [346/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [347/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [348/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [349/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [350/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [351/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [352/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [353/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [354/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [355/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [356/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [357/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [358/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [359/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [360/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [361/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [362/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [363/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [364/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [365/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [366/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [367/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [368/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [369/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [370/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [371/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [372/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [373/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [374/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [375/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [376/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [377/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [378/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [379/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [380/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [381/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [382/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [383/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [384/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [385/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [386/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [387/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [388/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [389/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [390/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [391/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [392/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [393/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [394/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [395/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [396/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [397/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [398/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [399/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [400/758], Val Loss: 0.0002\r",
      "Epoch [238/248], Step [401/758], Val Loss: 0.0003\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238/248], Step [402/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [403/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [404/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [405/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [406/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [407/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [408/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [409/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [410/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [411/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [412/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [413/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [414/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [415/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [416/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [417/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [418/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [419/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [420/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [421/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [422/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [423/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [424/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [425/758], Val Loss: 0.0022\r",
      "Epoch [238/248], Step [426/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [427/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [428/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [429/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [430/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [431/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [432/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [433/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [434/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [435/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [436/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [437/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [438/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [439/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [440/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [441/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [442/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [443/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [444/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [445/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [446/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [447/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [448/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [449/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [450/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [451/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [452/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [453/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [454/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [455/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [456/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [457/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [458/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [459/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [460/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [461/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [462/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [463/758], Val Loss: 0.0002\r",
      "Epoch [238/248], Step [464/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [465/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [466/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [467/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [468/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [469/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [470/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [471/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [472/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [473/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [474/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [475/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [476/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [477/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [478/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [479/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [480/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [481/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [482/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [483/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [484/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [485/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [486/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [487/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [488/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [489/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [490/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [491/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [492/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [493/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [494/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [495/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [496/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [497/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [498/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [499/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [500/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [501/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [502/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [503/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [504/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [505/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [506/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [507/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [508/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [509/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [510/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [511/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [512/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [513/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [514/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [515/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [516/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [517/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [518/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [519/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [520/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [521/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [522/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [523/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [524/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [525/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [526/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [527/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [528/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [529/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [530/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [531/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [532/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [533/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [534/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [535/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [536/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [537/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [538/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [539/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [540/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [541/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [542/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [543/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [544/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [545/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [546/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [547/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [548/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [549/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [550/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [551/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [552/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [553/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [554/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [555/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [556/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [557/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [558/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [559/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [560/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [561/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [562/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [563/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [564/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [565/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [566/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [567/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [568/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [569/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [570/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [571/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [572/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [573/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [574/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [575/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [576/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [577/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [578/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [579/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [580/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [581/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [582/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [583/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [584/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [585/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [586/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [587/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [588/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [589/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [590/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [591/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [592/758], Val Loss: 0.0000\r",
      "Epoch [238/248], Step [593/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [594/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [595/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [596/758], Val Loss: 0.0001\r",
      "Epoch [238/248], Step [597/758], Val Loss: 0.0001\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238/248], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [239/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [239/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 239\n",
      " at D:\\chk6\\239best_model.pth\n",
      "Epoch [240/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [240/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 240\n",
      " at D:\\chk6\\240best_model.pth\n",
      "Epoch [241/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [241/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 241\n",
      " at D:\\chk6\\241best_model.pth\n",
      "Epoch [242/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [242/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 242\n",
      " at D:\\chk6\\242best_model.pth\n",
      "Epoch [243/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [243/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 243\n",
      " at D:\\chk6\\243best_model.pth\n",
      "Epoch [244/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [244/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 244\n",
      " at D:\\chk6\\244best_model.pth\n",
      "Epoch [245/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [245/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 245\n",
      " at D:\\chk6\\245best_model.pth\n",
      "Epoch [246/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [246/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 246\n",
      " at D:\\chk6\\246best_model.pth\n",
      "Epoch [247/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [247/248], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [248/248], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [248/248], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 248\n",
      " at D:\\chk6\\248best_model.pth\n",
      "Time Consumed 789.4268169403076 sec\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54a01a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.3287696838378906 sec\n",
      "Mean Absolute Error (MAE): 89.88\n",
      "Median Absolute Error (MedAE): 70.36\n",
      "Mean Squared Error (MSE): 14538.16\n",
      "Root Mean Squared Error (RMSE): 120.57\n",
      "Mean Absolute Percentage Error (MAPE): 0.61 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.49 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk6\\248best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46d70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1d867c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "#lr = 0.0001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk5\\260best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b6d7fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fc6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [261/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [261/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 261\n",
      " at D:\\chk5\\261best_model.pth\n",
      "Epoch [262/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [262/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 262\n",
      " at D:\\chk5\\262best_model.pth\n",
      "Epoch [263/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [263/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 263\n",
      " at D:\\chk5\\263best_model.pth\n",
      "Epoch [264/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [264/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 264\n",
      " at D:\\chk5\\264best_model.pth\n",
      "Epoch [265/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [265/320], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [266/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [266/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 266\n",
      " at D:\\chk5\\266best_model.pth\n",
      "Epoch [267/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [267/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 267\n",
      " at D:\\chk5\\267best_model.pth\n",
      "Epoch [268/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [268/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 268\n",
      " at D:\\chk5\\268best_model.pth\n",
      "Epoch [269/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [269/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 269\n",
      " at D:\\chk5\\269best_model.pth\n",
      "Epoch [270/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [270/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 270\n",
      " at D:\\chk5\\270best_model.pth\n",
      "Epoch [271/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [271/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 271\n",
      " at D:\\chk5\\271best_model.pth\n",
      "Epoch [272/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [272/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 272\n",
      " at D:\\chk5\\272best_model.pth\n",
      "Epoch [273/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [273/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 273\n",
      " at D:\\chk5\\273best_model.pth\n",
      "Epoch [274/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [274/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 274\n",
      " at D:\\chk5\\274best_model.pth\n",
      "Epoch [275/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [275/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 275\n",
      " at D:\\chk5\\275best_model.pth\n",
      "Epoch [276/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [276/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 276\n",
      " at D:\\chk5\\276best_model.pth\n",
      "Epoch [277/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [277/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 277\n",
      " at D:\\chk5\\277best_model.pth\n",
      "Epoch [278/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [278/320], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [279/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [279/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 279\n",
      " at D:\\chk5\\279best_model.pth\n",
      "Epoch [280/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [280/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 280\n",
      " at D:\\chk5\\280best_model.pth\n",
      "Epoch [281/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [281/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 281\n",
      " at D:\\chk5\\281best_model.pth\n",
      "Epoch [282/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [282/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 282\n",
      " at D:\\chk5\\282best_model.pth\n",
      "Epoch [283/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [283/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 283\n",
      " at D:\\chk5\\283best_model.pth\n",
      "Epoch [284/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [284/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 284\n",
      " at D:\\chk5\\284best_model.pth\n",
      "Epoch [285/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [285/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 285\n",
      " at D:\\chk5\\285best_model.pth\n",
      "Epoch [286/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [286/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 286\n",
      " at D:\\chk5\\286best_model.pth\n",
      "Epoch [287/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [287/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 287\n",
      " at D:\\chk5\\287best_model.pth\n",
      "Epoch [288/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [288/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 288\n",
      " at D:\\chk5\\288best_model.pth\n",
      "Epoch [289/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [289/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 289\n",
      " at D:\\chk5\\289best_model.pth\n",
      "Epoch [290/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [290/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 290\n",
      " at D:\\chk5\\290best_model.pth\n",
      "Epoch [291/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [291/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 291\n",
      " at D:\\chk5\\291best_model.pth\n",
      "Epoch [292/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [292/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 292\n",
      " at D:\\chk5\\292best_model.pth\n",
      "Epoch [293/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [293/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 293\n",
      " at D:\\chk5\\293best_model.pth\n",
      "Epoch [294/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [294/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 294\n",
      " at D:\\chk5\\294best_model.pth\n",
      "Epoch [295/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [295/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 295\n",
      " at D:\\chk5\\295best_model.pth\n",
      "Epoch [296/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [296/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 296\n",
      " at D:\\chk5\\296best_model.pth\n",
      "Epoch [297/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [297/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 297\n",
      " at D:\\chk5\\297best_model.pth\n",
      "Epoch [298/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [298/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 298\n",
      " at D:\\chk5\\298best_model.pth\n",
      "Epoch [299/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [299/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 299\n",
      " at D:\\chk5\\299best_model.pth\n",
      "Epoch [300/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [300/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 300\n",
      " at D:\\chk5\\300best_model.pth\n",
      "Epoch [301/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [301/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 301\n",
      " at D:\\chk5\\301best_model.pth\n",
      "Epoch [302/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [302/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 302\n",
      " at D:\\chk5\\302best_model.pth\n",
      "Epoch [303/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [303/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 303\n",
      " at D:\\chk5\\303best_model.pth\n",
      "Epoch [304/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [304/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 304\n",
      " at D:\\chk5\\304best_model.pth\n",
      "Epoch [305/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [305/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 305\n",
      " at D:\\chk5\\305best_model.pth\n",
      "Epoch [306/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [306/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 306\n",
      " at D:\\chk5\\306best_model.pth\n",
      "Epoch [307/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [307/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 307\n",
      " at D:\\chk5\\307best_model.pth\n",
      "Epoch [308/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [308/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 308\n",
      " at D:\\chk5\\308best_model.pth\n",
      "Epoch [309/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [309/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 309\n",
      " at D:\\chk5\\309best_model.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [310/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [310/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 310\n",
      " at D:\\chk5\\310best_model.pth\n",
      "Epoch [311/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [311/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 311\n",
      " at D:\\chk5\\311best_model.pth\n",
      "Epoch [312/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [312/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 312\n",
      " at D:\\chk5\\312best_model.pth\n",
      "Epoch [313/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [313/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 313\n",
      " at D:\\chk5\\313best_model.pth\n",
      "Epoch [314/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [314/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 314\n",
      " at D:\\chk5\\314best_model.pth\n",
      "Epoch [315/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [315/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 315\n",
      " at D:\\chk5\\315best_model.pth\n",
      "Epoch [316/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [316/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 316\n",
      " at D:\\chk5\\316best_model.pth\n",
      "Epoch [317/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [317/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 317\n",
      " at D:\\chk5\\317best_model.pth\n",
      "Epoch [318/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [318/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 318\n",
      " at D:\\chk5\\318best_model.pth\n",
      "Epoch [319/320], Step [1288/2653], Training Loss: 0.0000\r"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7e146af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a820b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.39956188201904297 sec\n",
      "Mean Absolute Error (MAE): 89.36\n",
      "Median Absolute Error (MedAE): 69.47\n",
      "Mean Squared Error (MSE): 14358.5\n",
      "Root Mean Squared Error (RMSE): 119.83\n",
      "Mean Absolute Percentage Error (MAPE): 0.61 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.48 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk5\\318best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b1d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path=r'D:\\chk5'+str('\\\\') #Edit\n",
    "fig_path=r'D:\\chk5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbceab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "#lr = 0.0001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk5\\260best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae087b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "396ca99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [261/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [261/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 261\n",
      " at D:\\chk5\\261best_model.pth\n",
      "Epoch [262/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [262/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 262\n",
      " at D:\\chk5\\262best_model.pth\n",
      "Epoch [263/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [263/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 263\n",
      " at D:\\chk5\\263best_model.pth\n",
      "Epoch [264/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [264/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 264\n",
      " at D:\\chk5\\264best_model.pth\n",
      "Epoch [265/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [265/320], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [266/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [266/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 266\n",
      " at D:\\chk5\\266best_model.pth\n",
      "Epoch [267/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [267/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 267\n",
      " at D:\\chk5\\267best_model.pth\n",
      "Epoch [268/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [268/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 268\n",
      " at D:\\chk5\\268best_model.pth\n",
      "Epoch [269/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [269/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 269\n",
      " at D:\\chk5\\269best_model.pth\n",
      "Epoch [270/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [270/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 270\n",
      " at D:\\chk5\\270best_model.pth\n",
      "Epoch [271/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [271/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 271\n",
      " at D:\\chk5\\271best_model.pth\n",
      "Epoch [272/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [272/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 272\n",
      " at D:\\chk5\\272best_model.pth\n",
      "Epoch [273/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [273/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 273\n",
      " at D:\\chk5\\273best_model.pth\n",
      "Epoch [274/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [274/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 274\n",
      " at D:\\chk5\\274best_model.pth\n",
      "Epoch [275/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [275/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 275\n",
      " at D:\\chk5\\275best_model.pth\n",
      "Epoch [276/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [276/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 276\n",
      " at D:\\chk5\\276best_model.pth\n",
      "Epoch [277/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [277/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 277\n",
      " at D:\\chk5\\277best_model.pth\n",
      "Epoch [278/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [278/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 278\n",
      " at D:\\chk5\\278best_model.pth\n",
      "Epoch [279/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [279/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 279\n",
      " at D:\\chk5\\279best_model.pth\n",
      "Epoch [280/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [280/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 280\n",
      " at D:\\chk5\\280best_model.pth\n",
      "Epoch [281/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [281/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 281\n",
      " at D:\\chk5\\281best_model.pth\n",
      "Epoch [282/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [282/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 282\n",
      " at D:\\chk5\\282best_model.pth\n",
      "Epoch [283/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [283/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 283\n",
      " at D:\\chk5\\283best_model.pth\n",
      "Epoch [284/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [284/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 284\n",
      " at D:\\chk5\\284best_model.pth\n",
      "Epoch [285/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [285/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 285\n",
      " at D:\\chk5\\285best_model.pth\n",
      "Epoch [286/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [286/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 286\n",
      " at D:\\chk5\\286best_model.pth\n",
      "Epoch [287/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [287/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 287\n",
      " at D:\\chk5\\287best_model.pth\n",
      "Epoch [288/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [288/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 288\n",
      " at D:\\chk5\\288best_model.pth\n",
      "Epoch [289/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [289/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 289\n",
      " at D:\\chk5\\289best_model.pth\n",
      "Epoch [290/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [290/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 290\n",
      " at D:\\chk5\\290best_model.pth\n",
      "Epoch [291/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [291/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 291\n",
      " at D:\\chk5\\291best_model.pth\n",
      "Epoch [292/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [292/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 292\n",
      " at D:\\chk5\\292best_model.pth\n",
      "Epoch [293/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [293/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 293\n",
      " at D:\\chk5\\293best_model.pth\n",
      "Epoch [294/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [294/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 294\n",
      " at D:\\chk5\\294best_model.pth\n",
      "Epoch [295/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [295/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 295\n",
      " at D:\\chk5\\295best_model.pth\n",
      "Epoch [296/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [296/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 296\n",
      " at D:\\chk5\\296best_model.pth\n",
      "Epoch [297/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [297/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 297\n",
      " at D:\\chk5\\297best_model.pth\n",
      "Epoch [298/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [298/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 298\n",
      " at D:\\chk5\\298best_model.pth\n",
      "Epoch [299/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [299/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 299\n",
      " at D:\\chk5\\299best_model.pth\n",
      "Epoch [300/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [300/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 300\n",
      " at D:\\chk5\\300best_model.pth\n",
      "Epoch [301/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [301/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 301\n",
      " at D:\\chk5\\301best_model.pth\n",
      "Epoch [302/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [302/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 302\n",
      " at D:\\chk5\\302best_model.pth\n",
      "Epoch [303/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [303/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 303\n",
      " at D:\\chk5\\303best_model.pth\n",
      "Epoch [304/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [304/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 304\n",
      " at D:\\chk5\\304best_model.pth\n",
      "Epoch [305/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [305/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 305\n",
      " at D:\\chk5\\305best_model.pth\n",
      "Epoch [306/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [306/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 306\n",
      " at D:\\chk5\\306best_model.pth\n",
      "Epoch [307/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [307/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 307\n",
      " at D:\\chk5\\307best_model.pth\n",
      "Epoch [308/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [308/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 308\n",
      " at D:\\chk5\\308best_model.pth\n",
      "Epoch [309/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [309/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 309\n",
      " at D:\\chk5\\309best_model.pth\n",
      "Epoch [310/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [310/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 310\n",
      " at D:\\chk5\\310best_model.pth\n",
      "Epoch [311/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [311/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 311\n",
      " at D:\\chk5\\311best_model.pth\n",
      "Epoch [312/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [312/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 312\n",
      " at D:\\chk5\\312best_model.pth\n",
      "Epoch [313/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [313/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 313\n",
      " at D:\\chk5\\313best_model.pth\n",
      "Epoch [314/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [314/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 314\n",
      " at D:\\chk5\\314best_model.pth\n",
      "Epoch [315/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [315/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 315\n",
      " at D:\\chk5\\315best_model.pth\n",
      "Epoch [316/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [316/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 316\n",
      " at D:\\chk5\\316best_model.pth\n",
      "Epoch [317/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [317/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 317\n",
      " at D:\\chk5\\317best_model.pth\n",
      "Epoch [318/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [318/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 318\n",
      " at D:\\chk5\\318best_model.pth\n",
      "Epoch [319/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [319/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 319\n",
      " at D:\\chk5\\319best_model.pth\n",
      "Epoch [320/320], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [320/320], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 320\n",
      " at D:\\chk5\\320best_model.pth\n",
      "Time Consumed 777.062849521637 sec\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f0916d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.31932806968688965 sec\n",
      "Mean Absolute Error (MAE): 89.35\n",
      "Median Absolute Error (MedAE): 69.52\n",
      "Mean Squared Error (MSE): 14356.16\n",
      "Root Mean Squared Error (RMSE): 119.82\n",
      "Mean Absolute Percentage Error (MAPE): 0.61 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.49 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk5\\320best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a986ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecc47c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "#lr = 0.0001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk5\\320best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccdbf25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03133cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [321/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [321/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 321\n",
      " at D:\\chk5\\321best_model.pth\n",
      "Epoch [322/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [322/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 322\n",
      " at D:\\chk5\\322best_model.pth\n",
      "Epoch [323/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [323/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 323\n",
      " at D:\\chk5\\323best_model.pth\n",
      "Epoch [324/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [324/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 324\n",
      " at D:\\chk5\\324best_model.pth\n",
      "Epoch [325/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [325/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 325\n",
      " at D:\\chk5\\325best_model.pth\n",
      "Epoch [326/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [326/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 326\n",
      " at D:\\chk5\\326best_model.pth\n",
      "Epoch [327/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [327/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 327\n",
      " at D:\\chk5\\327best_model.pth\n",
      "Epoch [328/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [328/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 328\n",
      " at D:\\chk5\\328best_model.pth\n",
      "Epoch [329/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [329/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 329\n",
      " at D:\\chk5\\329best_model.pth\n",
      "Epoch [330/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [330/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 330\n",
      " at D:\\chk5\\330best_model.pth\n",
      "Epoch [331/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [331/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 331\n",
      " at D:\\chk5\\331best_model.pth\n",
      "Epoch [332/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [332/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 332\n",
      " at D:\\chk5\\332best_model.pth\n",
      "Epoch [333/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [333/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 333\n",
      " at D:\\chk5\\333best_model.pth\n",
      "Epoch [334/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [334/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 334\n",
      " at D:\\chk5\\334best_model.pth\n",
      "Epoch [335/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [335/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 335\n",
      " at D:\\chk5\\335best_model.pth\n",
      "Epoch [336/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [336/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 336\n",
      " at D:\\chk5\\336best_model.pth\n",
      "Epoch [337/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [337/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 337\n",
      " at D:\\chk5\\337best_model.pth\n",
      "Epoch [338/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [338/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 338\n",
      " at D:\\chk5\\338best_model.pth\n",
      "Epoch [339/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [339/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 339\n",
      " at D:\\chk5\\339best_model.pth\n",
      "Epoch [340/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [340/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 340\n",
      " at D:\\chk5\\340best_model.pth\n",
      "Epoch [341/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [341/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 341\n",
      " at D:\\chk5\\341best_model.pth\n",
      "Epoch [342/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [342/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 342\n",
      " at D:\\chk5\\342best_model.pth\n",
      "Epoch [343/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [343/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 343\n",
      " at D:\\chk5\\343best_model.pth\n",
      "Epoch [344/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [344/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 344\n",
      " at D:\\chk5\\344best_model.pth\n",
      "Epoch [345/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [345/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 345\n",
      " at D:\\chk5\\345best_model.pth\n",
      "Epoch [346/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [346/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 346\n",
      " at D:\\chk5\\346best_model.pth\n",
      "Epoch [347/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [347/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 347\n",
      " at D:\\chk5\\347best_model.pth\n",
      "Epoch [348/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [348/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 348\n",
      " at D:\\chk5\\348best_model.pth\n",
      "Epoch [349/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [349/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 349\n",
      " at D:\\chk5\\349best_model.pth\n",
      "Epoch [350/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [350/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 350\n",
      " at D:\\chk5\\350best_model.pth\n",
      "Epoch [351/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [351/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 351\n",
      " at D:\\chk5\\351best_model.pth\n",
      "Epoch [352/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [352/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 352\n",
      " at D:\\chk5\\352best_model.pth\n",
      "Epoch [353/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [353/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 353\n",
      " at D:\\chk5\\353best_model.pth\n",
      "Epoch [354/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [354/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 354\n",
      " at D:\\chk5\\354best_model.pth\n",
      "Epoch [355/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [355/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 355\n",
      " at D:\\chk5\\355best_model.pth\n",
      "Epoch [356/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [356/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 356\n",
      " at D:\\chk5\\356best_model.pth\n",
      "Epoch [357/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [357/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 357\n",
      " at D:\\chk5\\357best_model.pth\n",
      "Epoch [358/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [358/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 358\n",
      " at D:\\chk5\\358best_model.pth\n",
      "Epoch [359/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [359/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 359\n",
      " at D:\\chk5\\359best_model.pth\n",
      "Epoch [360/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [360/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 360\n",
      " at D:\\chk5\\360best_model.pth\n",
      "Epoch [361/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [361/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 361\n",
      " at D:\\chk5\\361best_model.pth\n",
      "Epoch [362/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [362/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 362\n",
      " at D:\\chk5\\362best_model.pth\n",
      "Epoch [363/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [363/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 363\n",
      " at D:\\chk5\\363best_model.pth\n",
      "Epoch [364/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [364/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 364\n",
      " at D:\\chk5\\364best_model.pth\n",
      "Epoch [365/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [365/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 365\n",
      " at D:\\chk5\\365best_model.pth\n",
      "Epoch [366/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [366/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 366\n",
      " at D:\\chk5\\366best_model.pth\n",
      "Epoch [367/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [367/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 367\n",
      " at D:\\chk5\\367best_model.pth\n",
      "Epoch [368/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [368/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 368\n",
      " at D:\\chk5\\368best_model.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [369/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [369/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 369\n",
      " at D:\\chk5\\369best_model.pth\n",
      "Epoch [370/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [370/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 370\n",
      " at D:\\chk5\\370best_model.pth\n",
      "Epoch [371/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [371/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 371\n",
      " at D:\\chk5\\371best_model.pth\n",
      "Epoch [372/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [372/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 372\n",
      " at D:\\chk5\\372best_model.pth\n",
      "Epoch [373/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [373/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 373\n",
      " at D:\\chk5\\373best_model.pth\n",
      "Epoch [374/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [374/380], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [375/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [375/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 375\n",
      " at D:\\chk5\\375best_model.pth\n",
      "Epoch [376/380], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [376/380], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 376\n",
      " at D:\\chk5\\376best_model.pth\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9078f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f82ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.43485236167907715 sec\n",
      "Mean Absolute Error (MAE): 89.15\n",
      "Median Absolute Error (MedAE): 69.46\n",
      "Mean Squared Error (MSE): 14294.5\n",
      "Root Mean Squared Error (RMSE): 119.56\n",
      "Mean Absolute Percentage Error (MAPE): 0.61 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.48 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk5\\376best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "381f8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path=r'D:\\chk5'+str('\\\\') #Edit\n",
    "fig_path=r'D:\\chk5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fbc3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "#lr = 0.0001\n",
    "Batch_size=32\n",
    "load_model_path=r'D:\\chk5\\436best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9658453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n"
     ]
    }
   ],
   "source": [
    "if Batch_size is True:\n",
    "    model_, optimizer, start_epoch= load_model(model, model_path=load_model_path,\n",
    "                               lr=lr,\n",
    "                               train_X=train_X,\n",
    "                               train_y=train_y,\n",
    "                               validation_X=validation_X,\n",
    "                               validation_y=validation_y,\n",
    "                               test_X=test_X,\n",
    "                               test_y=test_y)\n",
    "\n",
    "else:\n",
    "    model_, optimizer, start_epoch, train_data_loader, validation_data_loader, test_data_loader = load_model(model,model_path=load_model_path,\n",
    "                                                                                             lr=lr,\n",
    "                                                                                             Batch_Size=Batch_size,\n",
    "                                                                                             train_X=train_X,\n",
    "                                                                                             train_y=train_y,\n",
    "                                                                                             validation_X=validation_X,\n",
    "                                                                                             validation_y=validation_y,\n",
    "                                                                                             test_X=test_X,\n",
    "                                                                                             test_y=test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6a27361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [437/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [437/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 437\n",
      " at D:\\chk5\\437best_model.pth\n",
      "Epoch [438/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [438/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 438\n",
      " at D:\\chk5\\438best_model.pth\n",
      "Epoch [439/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [439/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [440/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [440/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 440\n",
      " at D:\\chk5\\440best_model.pth\n",
      "Epoch [441/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [441/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 441\n",
      " at D:\\chk5\\441best_model.pth\n",
      "Epoch [442/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [442/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [443/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [443/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 443\n",
      " at D:\\chk5\\443best_model.pth\n",
      "Epoch [444/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [444/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 444\n",
      " at D:\\chk5\\444best_model.pth\n",
      "Epoch [445/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [445/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 445\n",
      " at D:\\chk5\\445best_model.pth\n",
      "Epoch [446/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [446/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [447/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [447/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 447\n",
      " at D:\\chk5\\447best_model.pth\n",
      "Epoch [448/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [448/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 448\n",
      " at D:\\chk5\\448best_model.pth\n",
      "Epoch [449/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [449/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 449\n",
      " at D:\\chk5\\449best_model.pth\n",
      "Epoch [450/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [450/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 450\n",
      " at D:\\chk5\\450best_model.pth\n",
      "Epoch [451/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [451/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 451\n",
      " at D:\\chk5\\451best_model.pth\n",
      "Epoch [452/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [452/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [453/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [453/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 453\n",
      " at D:\\chk5\\453best_model.pth\n",
      "Epoch [454/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [454/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 454\n",
      " at D:\\chk5\\454best_model.pth\n",
      "Epoch [455/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [455/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [456/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [456/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 456\n",
      " at D:\\chk5\\456best_model.pth\n",
      "Epoch [457/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [457/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 457\n",
      " at D:\\chk5\\457best_model.pth\n",
      "Epoch [458/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [458/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 458\n",
      " at D:\\chk5\\458best_model.pth\n",
      "Epoch [459/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [459/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 459\n",
      " at D:\\chk5\\459best_model.pth\n",
      "Epoch [460/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [460/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 460\n",
      " at D:\\chk5\\460best_model.pth\n",
      "Epoch [461/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [461/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [462/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [462/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 462\n",
      " at D:\\chk5\\462best_model.pth\n",
      "Epoch [463/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [463/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 463\n",
      " at D:\\chk5\\463best_model.pth\n",
      "Epoch [464/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [464/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [465/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [465/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 465\n",
      " at D:\\chk5\\465best_model.pth\n",
      "Epoch [466/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [466/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 466\n",
      " at D:\\chk5\\466best_model.pth\n",
      "Epoch [467/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [467/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 467\n",
      " at D:\\chk5\\467best_model.pth\n",
      "Epoch [468/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [468/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 468\n",
      " at D:\\chk5\\468best_model.pth\n",
      "Epoch [469/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [469/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 469\n",
      " at D:\\chk5\\469best_model.pth\n",
      "Epoch [470/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [470/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 470\n",
      " at D:\\chk5\\470best_model.pth\n",
      "Epoch [471/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [471/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 471\n",
      " at D:\\chk5\\471best_model.pth\n",
      "Epoch [472/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [472/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [473/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [473/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 473\n",
      " at D:\\chk5\\473best_model.pth\n",
      "Epoch [474/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [474/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 474\n",
      " at D:\\chk5\\474best_model.pth\n",
      "Epoch [475/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [475/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 475\n",
      " at D:\\chk5\\475best_model.pth\n",
      "Epoch [476/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [476/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 476\n",
      " at D:\\chk5\\476best_model.pth\n",
      "Epoch [477/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [477/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 477\n",
      " at D:\\chk5\\477best_model.pth\n",
      "Epoch [478/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [478/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 478\n",
      " at D:\\chk5\\478best_model.pth\n",
      "Epoch [479/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [479/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 479\n",
      " at D:\\chk5\\479best_model.pth\n",
      "Epoch [480/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [480/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 480\n",
      " at D:\\chk5\\480best_model.pth\n",
      "Epoch [481/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [481/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 481\n",
      " at D:\\chk5\\481best_model.pth\n",
      "Epoch [482/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [482/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 482\n",
      " at D:\\chk5\\482best_model.pth\n",
      "Epoch [483/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [483/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 483\n",
      " at D:\\chk5\\483best_model.pth\n",
      "Epoch [484/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [484/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 484\n",
      " at D:\\chk5\\484best_model.pth\n",
      "Epoch [485/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [485/496], Step [758/758], Val Loss: 0.0001\n",
      "Epoch [486/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [486/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 486\n",
      " at D:\\chk5\\486best_model.pth\n",
      "Epoch [487/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [487/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 487\n",
      " at D:\\chk5\\487best_model.pth\n",
      "Epoch [488/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [488/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 488\n",
      " at D:\\chk5\\488best_model.pth\n",
      "Epoch [489/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [489/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 489\n",
      " at D:\\chk5\\489best_model.pth\n",
      "Epoch [490/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [490/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 490\n",
      " at D:\\chk5\\490best_model.pth\n",
      "Epoch [491/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [491/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 491\n",
      " at D:\\chk5\\491best_model.pth\n",
      "Epoch [492/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [492/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 492\n",
      " at D:\\chk5\\492best_model.pth\n",
      "Epoch [493/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [493/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 493\n",
      " at D:\\chk5\\493best_model.pth\n",
      "Epoch [494/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [494/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 494\n",
      " at D:\\chk5\\494best_model.pth\n",
      "Epoch [495/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [495/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 495\n",
      " at D:\\chk5\\495best_model.pth\n",
      "Epoch [496/496], Step [2653/2653], Training Loss: 0.0000\n",
      "Epoch [496/496], Step [758/758], Val Loss: 0.0001\n",
      "\n",
      "Saving best model for epoch: 496\n",
      " at D:\\chk5\\496best_model.pth\n",
      "Time Consumed 769.2394304275513 sec\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_)\n",
    "start = time.time()\n",
    "\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader,\n",
    "     load_model_epoch=start_epoch)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ccc055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 1e-05\n",
      "Time Consumed 0.2789039611816406 sec\n",
      "Mean Absolute Error (MAE): 88.81\n",
      "Median Absolute Error (MedAE): 69.05\n",
      "Mean Squared Error (MSE): 14186.4\n",
      "Root Mean Squared Error (RMSE): 119.11\n",
      "Mean Absolute Percentage Error (MAPE): 0.61 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.48 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'D:\\chk5\\496best_model.pth'\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2aece3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa994278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ea731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e3773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a19ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104c35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5e208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58f1a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\personal\\.conda\\envs\\pyt2\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff78c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0487add0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4fbe94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9112b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ff80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
